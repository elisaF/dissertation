\chapter{Conclusion}
\label{ch:conclusion}

Computational discourse tackles meaning beyond the sentence to understand how a document is organized and to study what makes it both cohesive and coherent. 

In this dissertation, I first examined how to extract discourse from longer texts for use in computational models. I found that cohesive devices in the form of coreference chains were useful for identifying salient information in abstracts of medical reports. For the task of identifying the author of novel excerpts, I incorporated embeddings of discourse relation sequences into a neural network to model the progression of coherence for entities in the excerpt, reflecting particular author writing styles.

While these forms of abstraction were somewhat effective, they were also limited by the lack of annotated data that could help them scale better. In a low-resource setting, unsupervised learning is an attractive alternative, but I found that learned structures were not representative of discourse, showing that the complex inferences in discourse require stronger signals and guidance to learn. 

I instead attempted a supervised approach to manually annotate discourse in a new domain. The first step of identifying the discourse units was successful and yielded insights for cross-domain applications. However, building the discourse trees resulted in conflicting yet valid structures. This result prompted a reevaluation of discourse as being embedded in a social context, and thus being subjective.

My focus on subjective discourse led me to create the first corpus in English with subjective judgments of discourse in the form of perceived conversation acts and intents. I found that disagreements on these labels were genuine and frequent, but couldn't be easily explained by any cues in the text or biases of the annotators. I concluded that I have only begun to scratch the surface of understanding these disagreements.

\section{Future Work}
My work on subjective discourse sparked several possible research directions, which I briefly discuss here. 

\paragraph{All possible interpretations + Level of disagreement} Preliminary results on the task of predicting the level of disagreement show an MSE objective with regression yields poor performance. An ordinal regression can be explored, which could then be combined with the multi-label classification task so that knowing all the possible interpretations of a response would help predict the level of disagreement on those labels. 

\paragraph{Sequential model}
In my work, I treated the conversation act and intent labels as temporally independent. In reality, they stem from conversations spanning at least ten turns. The labels should be modeled as sequences, much like dialog acts. For annotator biases, sequential patterns likely also exist as prior work has shown it is harder for people to move from negative to positive sentiment vs positive to negative \cite{Boydstun:2019}. A study in economics also suggests people may be updating their prior beliefs twice (once to interpret the current turn, and again to form a posterior over the sequence of turns) which results in people getting more polarized as they see more of the same signal \cite{Fryer:2019}. Furthermore, a sequential model would allow me to incorporate other features of conversation acts that include turn taking, where turn-grabbing (as marked by `--' when a witness is cut off) is already a strong signal that the responder is shifting the question (refer to the local mutual information analysis in Appendix \ref{sec:app_lmi}). 


\paragraph{Multimodal model}
The congressional hearings from which I extract my data also have audio and video that I could incorporate into a sequence multi-labeling model as pioneered in \cite{Li:2010}. Incorporating these modalities could provide additional information for determining the conversation act and intent. Prosodic features are objectively different in non-cooperative dialogues compared to cooperative ones, such as longer turn taking latencies (a signal of increased cognitive processing effort) and stronger F0 declinations (a signal for early turn termination) \cite{Reichel:2018}. However, these additional modalities could also introduce new sources of bias. For example, observers are prone to assume deception when a person averts their gaze, even though actual lying is not correlated with this action \cite{Levine:2019}.

\paragraph{Annotation with finer-scoped and graded labels}
A new version of the annotation task could be created to address two current shortcomings. The first issue is that we force the annotator to choose a single label for an entire (often lengthy) utterance, when in fact a response can be judged to have multiple, \emph{simultaneous} intents especially since multiple questions can be asked (for example, a response could provide a direct answer to one question, but also try to evade another one). The second issue is that annotators must make binary yes/no choices without room for gray area. Many phenomena in NLP are instead judged on a graded scale, recognizing that one label may not fully apply (such as word sense disambiguation in \newcite{Erk:2009}).

To overcome the sometimes over-simplifying approach of utterance-level labels, we would ask annotators to label at the sentence or clause level. An utterance-level label could then be inferred by composing the sentence/clause-level labels in a hierarchical models. Alternatively, the multiple labels could be modeled as a mixture for each annotator, as in \newcite{Jo:2017} which employs an unsupervised approach to model utterances as having a mixture of multiple dialog acts. To remedy the forced binary choice, the revised annotation task would either employ sliders to gauge how much a certain label applies, or a series of binary choice labels to convey the degree to which an annotator feels the label applies (similar to \newcite{Pluss:2014}). 

\paragraph{Other languages}
How speakers express their intents and how annotators express their interpretation of intents (and indirectly their attitudes towards the speakers) may vary across languages. In English, judgments are expressed mainly through lexical means (such as using hedges or modals). However, in other languages these phenomena are often grammaticalized, allowing for a more rigorous study. For example, Japanese has sentential evidential markers to distinguish between reporting an experience or seeking approval (`desu-\textit{yo}' vs. `desu-\textit{ne}') \cite{Stojanovic:2019}. %and Chinese has two causal connectives that can signal either subjectivity or be underspecified (kejian vs. suoyi) \cite{Wei:2020}.